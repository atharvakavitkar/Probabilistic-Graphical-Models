{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from models import dynamic_lda\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer=RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5de193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "\n",
    "dataset_dir = \"D:/Masters/4/Probabilistic Graphical Models/data/\" #set the directory of dataset\n",
    "\n",
    "num_articles = 10 #Number of news articles to be considered from each time slice\n",
    "\n",
    "yearwise = True #Select between yearwise or decadewise analysis (yearwise is faster than decadewise)\n",
    "\n",
    "start_year = 2016 #choose start year in range of years to be analysed\n",
    "\n",
    "end_year = 2018 #choose end year (both inclusive range)\n",
    "\n",
    "num_topics = 5 #number of topics to be found in the dataset\n",
    "\n",
    "#increasing the num_articles and year range will increase model training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct dataframe from csv files\n",
    "\n",
    "def data_construction(dataset_dir,yearwise):\n",
    "    year_files = glob.glob(os.path.join(dataset_dir, \"*.csv\"))\n",
    "    df = pd.DataFrame()\n",
    "    if yearwise:\n",
    "        years = []\n",
    "        articles_per_time = []\n",
    "        for x in range(start_year,end_year+1):\n",
    "            years.append(str(x))\n",
    "            articles_per_time.append(num_articles)\n",
    "\n",
    "        for file in year_files:\n",
    "            if any(year in file for year in years):\n",
    "                year_df = pd.read_csv(file,index_col = 0)\n",
    "                year_df = year_df.rename({'sentence': 'article'}, axis=1).drop_duplicates()\n",
    "                year_df = year_df.sort_values(by=\"article\", key=lambda x: x.str.len(),ignore_index=True, ascending=False)[:num_articles]\n",
    "                df = df.append(year_df)\n",
    "    else:\n",
    "        decades = []\n",
    "        articles_per_time = []\n",
    "        for decade_firstyear in range(0,len(year_files),10):\n",
    "            decade = year_files[decade_firstyear:decade_firstyear+10]\n",
    "            decade_df = pd.DataFrame()\n",
    "            years = []\n",
    "            for year in decade:\n",
    "                years.append(pd.read_csv(year,index_col = 0))\n",
    "            decade_df = pd.concat(years, ignore_index=True).drop_duplicates()\n",
    "            decade_df = decade_df.sort_values(by=\"sentence\", key=lambda x: x.str.len(),ignore_index=True, ascending=False)[:num_articles]\n",
    "            decades.append(decade_df)\n",
    "            articles_per_time.append(num_articles)\n",
    "        df = pd.concat(decades, ignore_index=True)\n",
    "        df = df.rename({'sentence': 'article'}, axis=1)\n",
    "\n",
    "    df = df.reset_index().drop(columns=['index'])\n",
    "    return df,articles_per_time\n",
    "\n",
    "df,articles_per_time = data_construction(dataset_dir,yearwise)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f367d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions for data preprocessing\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) \n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        texts_out.append([lemmatizer.lemmatize(w) for w in sent])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "\n",
    "def data_preprocessing(df):\n",
    "    # Convert to list\n",
    "    data = df.article.values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "    \n",
    "    data_words = list(sent_to_words(data))\n",
    "    \n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    data_words_bigrams = [bigram_mod[doc] for doc in data_words_nostops]\n",
    "    data_words_trigrams = [trigram_mod[bigram_mod[doc]] for doc in data_words_bigrams]\n",
    "    \n",
    "    data_lemmatized = lemmatization(data_words_trigrams)\n",
    "    \n",
    "    return data_lemmatized\n",
    "\n",
    "preprocessed_data = data_preprocessing(df)\n",
    "preprocessed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary and corpus\n",
    "\n",
    "def get_corpus_id2word(data):\n",
    "    id2word = corpora.Dictionary(data)\n",
    "    corpus = [id2word.doc2bow(text) for text in data]\n",
    "    return corpus,id2word\n",
    "\n",
    "corpus,id2word = get_corpus_id2word(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0085d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the dynamic_lda_model\n",
    "\n",
    "dyn_lda = dynamic_lda.Dynamic_LdaModel(corpus=corpus, id2word=id2word, \n",
    "                                       articles_per_time = articles_per_time, \n",
    "                                       num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6107493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_topic function provides the top 20 words given topic and time slice\n",
    "\n",
    "print(dyn_lda.get_topic(topic=0,time=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
